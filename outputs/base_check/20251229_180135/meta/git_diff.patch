diff --git a/outputs/sweep/20251229_102355/meta/commandline.txt b/outputs/sweep/20251229_102355/meta/commandline.txt
deleted file mode 100644
index 79775fb..0000000
--- a/outputs/sweep/20251229_102355/meta/commandline.txt
+++ /dev/null
@@ -1 +0,0 @@
-scripts/sweep_stepC.py --base-config configs/train_plateau.yaml --outer-iters 200 --grid 2 --jobs 1
\ No newline at end of file
diff --git a/outputs/sweep/20251229_102355/meta/grid_effective.yaml b/outputs/sweep/20251229_102355/meta/grid_effective.yaml
deleted file mode 100644
index 47fc7c3..0000000
--- a/outputs/sweep/20251229_102355/meta/grid_effective.yaml
+++ /dev/null
@@ -1,13 +0,0 @@
-beta_values:
-- 0.005
-- 0.4
-alpha_w_values:
-- 0.02
-- 0.24
-offpolicy_axis: sigma_mu
-offpolicy_values:
-- 0.175
-- 0.7
-seed_values:
-- 0
-base_config: configs/train_plateau.yaml
diff --git a/outputs/sweep/20251229_102355/meta/meta.json b/outputs/sweep/20251229_102355/meta/meta.json
deleted file mode 100644
index d9210f7..0000000
--- a/outputs/sweep/20251229_102355/meta/meta.json
+++ /dev/null
@@ -1,12 +0,0 @@
-{
-  "timestamp": "2025-12-29T10:23:55",
-  "command": "scripts/sweep_stepC.py --base-config configs/train_plateau.yaml --outer-iters 200 --grid 2 --jobs 1",
-  "git": {
-    "commit": "a84b364b797739f6a6669e4a8308245a452d798f",
-    "dirty": true
-  },
-  "python": {
-    "executable": "/Library/Developer/CommandLineTools/usr/bin/python3",
-    "version": "3.9.6 (default, Apr 30 2025, 02:07:17)  [Clang 17.0.0 (clang-1700.0.13.5)]"
-  }
-}
\ No newline at end of file
diff --git a/outputs/sweep/20251229_102355/meta/run_plan.jsonl b/outputs/sweep/20251229_102355/meta/run_plan.jsonl
deleted file mode 100644
index e69de29..0000000
diff --git a/tdrl_unfixed_ac/algos/train_unfixed_ac.py b/tdrl_unfixed_ac/algos/train_unfixed_ac.py
index fd221c2..1ddcab3 100644
--- a/tdrl_unfixed_ac/algos/train_unfixed_ac.py
+++ b/tdrl_unfixed_ac/algos/train_unfixed_ac.py
@@ -15,6 +15,7 @@ import numpy as np
 from tdrl_unfixed_ac.algos.unfixed_ac import (
     LinearGaussianPolicy,
     apply_rho_clip,
+    batch_step_scale,
     critic_value,
     project_to_ball,
 )
@@ -45,6 +46,10 @@ DEFAULT_TRAIN_CONFIG: Dict[str, Any] = {
     "theta_radius": 4.0,
     "theta_init_scale": 0.1,
     "w_init_scale": 0.1,
+    "theta_mu_offset_scale": 0.0,
+    "squash_action": False,
+    "log_contract_metrics": False,
+    "require_teacher_reward": True,
     "rho_clip": None,
     "disable_rho_clip": False,
     "checkpoint_every": 5,
@@ -113,8 +118,8 @@ def load_train_config(path: Optional[str] = None) -> Dict[str, Any]:
     return config
 
 
-def _clip_action(action: np.ndarray, v_max: float) -> np.ndarray:
-    if v_max <= 0.0:
+def _clip_action(action: np.ndarray, v_max: float, clip_action: bool) -> np.ndarray:
+    if not clip_action or v_max <= 0.0:
         return action
     return np.clip(action, -v_max, v_max)
 
@@ -136,6 +141,23 @@ def _mc_bar_phi(
     return np.mean(np.stack(phis, axis=0), axis=0)
 
 
+def _gaussian_kl_isotropic(
+    mean_p: np.ndarray,
+    sigma_p: float,
+    mean_q: np.ndarray,
+    sigma_q: float,
+) -> np.ndarray:
+    mean_p = np.asarray(mean_p, dtype=float)
+    mean_q = np.asarray(mean_q, dtype=float)
+    var_p = float(sigma_p) ** 2
+    var_q = float(sigma_q) ** 2
+    diff = mean_q - mean_p
+    diff_norm_sq = np.sum(diff * diff, axis=-1)
+    dim = mean_p.shape[-1]
+    log_ratio = np.log(var_q / var_p)
+    return 0.5 * (dim * (var_p / var_q) + diff_norm_sq / var_q - dim + dim * log_ratio)
+
+
 def _json_ready(obj: Any) -> Any:
     if isinstance(obj, np.ndarray):
         return obj.tolist()
@@ -191,6 +213,10 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
     theta_radius = float(cfg.get("theta_radius", 0.0))
     theta_init_scale = float(cfg.get("theta_init_scale", 0.1))
     w_init_scale = float(cfg.get("w_init_scale", 0.1))
+    theta_mu_offset_scale = float(cfg.get("theta_mu_offset_scale", 0.0))
+    squash_action = bool(cfg.get("squash_action", False))
+    log_contract_metrics = bool(cfg.get("log_contract_metrics", False))
+    require_teacher_reward = bool(cfg.get("require_teacher_reward", True))
     rho_clip = cfg.get("rho_clip", None)
     disable_rho_clip = bool(cfg.get("disable_rho_clip", False))
     checkpoint_every = int(cfg.get("checkpoint_every", 0))
@@ -200,8 +226,8 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
     output_dir = Path(cfg.get("output_dir", "outputs/unfixed_ac"))
     resume = bool(cfg.get("resume", False))
     resume_from = cfg.get("resume_from", None)
-    total_steps = max(trajectories * horizon, 1)
-    train_step_scale = alpha_w / total_steps
+    step_scale = batch_step_scale(trajectories, horizon)
+    train_step_scale = alpha_w * step_scale
 
     output_dir.mkdir(parents=True, exist_ok=True)
     checkpoint_dir = output_dir / "checkpoints"
@@ -217,6 +243,8 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
         if env_cfg.get("seed") is None:
             env_cfg["seed"] = seed
         env = TorusGobletGhostEnv(config=env_cfg)
+        if require_teacher_reward and not env.use_teacher_reward:
+            raise ValueError("Teacher reward required: set env.use_teacher_reward=True.")
 
         seeder = Seeder(seed)
         init_rng = seeder.spawn()
@@ -228,6 +256,8 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
 
         theta_pi = init_rng.normal(loc=0.0, scale=theta_init_scale, size=(actor_dim, action_dim))
         theta_mu = np.array(theta_pi, copy=True)
+        if theta_mu_offset_scale > 0.0:
+            theta_mu += init_rng.normal(loc=0.0, scale=theta_mu_offset_scale, size=theta_mu.shape)
         w = init_rng.normal(loc=0.0, scale=w_init_scale, size=feature_dim)
 
         logs = _load_existing_logs(csv_path)
@@ -264,6 +294,7 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
             k_mc=k_mc,
             sigma_mu=sigma_mu,
             sigma_pi=sigma_pi,
+            squash_action=squash_action,
             rho_clip=rho_clip,
             disable_rho_clip=disable_rho_clip,
         )
@@ -286,8 +317,18 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
         last_report_time = time.time()
 
         for n in range(start_iter, outer_iters):
-            mu_policy = LinearGaussianPolicy(theta=theta_mu, sigma=sigma_mu, v_max=env.v_max)
-            pi_policy = LinearGaussianPolicy(theta=theta_pi, sigma=sigma_pi, v_max=env.v_max)
+            mu_policy = LinearGaussianPolicy(
+                theta=theta_mu,
+                sigma=sigma_mu,
+                v_max=env.v_max,
+                squash_action=squash_action,
+            )
+            pi_policy = LinearGaussianPolicy(
+                theta=theta_pi,
+                sigma=sigma_pi,
+                v_max=env.v_max,
+                squash_action=squash_action,
+            )
 
             grad_w = np.zeros_like(w)
             grad_theta = np.zeros_like(theta_pi)
@@ -297,6 +338,10 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
             rho_exec_vals = []
             a_diff_vals = []
             clip_count = 0
+            action_sum = np.zeros(action_dim, dtype=float)
+            action_sq_sum = np.zeros(action_dim, dtype=float)
+            action_count = 0
+            psi_samples = [] if log_contract_metrics else None
 
             delta_cache = None
             if probe_manager.enabled and probe_manager.q_kernel_enabled:
@@ -312,11 +357,16 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
                 for t_idx in range(horizon):
                     # ---- sample action from behavior policy mu ----
                     a_exec = mu_policy.sample_action(psi, rollout_rng)
-                    a_clip = _clip_action(a_exec, env.v_max)
+                    a_clip = _clip_action(a_exec, env.v_max, env.clip_action)
                     a_diff = float(np.linalg.norm(a_exec - a_clip))
                     a_diff_vals.append(a_diff)
                     if a_diff > 1e-12:
                         clip_count += 1
+                    if log_contract_metrics:
+                        action_sum += a_exec
+                        action_sq_sum += a_exec * a_exec
+                        action_count += 1
+                        psi_samples.append(np.array(psi, copy=True))
 
                     # ---- importance ratio rho = pi(a|s) / mu(a|s) ----
                     u_exec = mu_policy.pre_squash(a_exec)
@@ -365,9 +415,8 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
             w_prev = np.array(w, copy=True)
             theta_pi_prev = np.array(theta_pi, copy=True)
 
-            scale = 1.0 / total_steps
-            w = w + alpha_w * scale * grad_w
-            theta_pi = theta_pi + alpha_pi * scale * grad_theta
+            w = w + alpha_w * step_scale * grad_w
+            theta_pi = theta_pi + alpha_pi * step_scale * grad_theta
             theta_mu = (1.0 - beta) * theta_mu + beta * theta_pi
 
             theta_pi = project_to_ball(theta_pi, theta_radius)
@@ -443,6 +492,43 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
             critic_teacher_error = float(np.dot(w - teacher_w, w - teacher_w) / feature_dim)
             tracking_gap = float(np.linalg.norm(theta_pi - theta_mu) ** 2 / actor_dim)
             w_norm = float(np.linalg.norm(w))
+            extra_metrics = {}
+            if log_contract_metrics:
+                w_norm_contract = float(np.linalg.norm(w) / np.sqrt(feature_dim))
+                theta_pi_norm = float(np.linalg.norm(theta_pi) / np.sqrt(actor_dim))
+                theta_mu_norm = float(np.linalg.norm(theta_mu) / np.sqrt(actor_dim))
+                tracking_gap_contract = float(np.linalg.norm(theta_pi - theta_mu) / np.sqrt(actor_dim))
+                w_dot_wr_over_n = float(np.dot(w, teacher_w) / feature_dim)
+                w_norm_denom = float(np.linalg.norm(teacher_w) * np.linalg.norm(w))
+                cos_w_wr = float(np.dot(w, teacher_w) / w_norm_denom) if w_norm_denom > 0 else float("nan")
+                if action_count > 0:
+                    mean_vec = action_sum / action_count
+                    var_vec = action_sq_sum / action_count - mean_vec * mean_vec
+                    action_mean = float(np.mean(mean_vec))
+                    action_var = float(np.mean(var_vec))
+                else:
+                    action_mean = float("nan")
+                    action_var = float("nan")
+                dist_action_kl = float("nan")
+                if psi_samples:
+                    psi_batch = np.stack(psi_samples, axis=0)
+                    mean_pi = (psi_batch @ theta_pi) / np.sqrt(actor_dim)
+                    mean_mu = (psi_batch @ theta_mu) / np.sqrt(actor_dim)
+                    kl_vals = _gaussian_kl_isotropic(mean_pi, sigma_pi, mean_mu, sigma_mu)
+                    if kl_vals.size:
+                        dist_action_kl = float(np.mean(kl_vals))
+                extra_metrics = {
+                    "w_norm_contract": w_norm_contract,
+                    "theta_pi_norm": theta_pi_norm,
+                    "theta_mu_norm": theta_mu_norm,
+                    "tracking_gap_contract": tracking_gap_contract,
+                    "w_dot_wr_over_n": w_dot_wr_over_n,
+                    "cos_w_wr": cos_w_wr,
+                    "action_mean": action_mean,
+                    "action_var": action_var,
+                    "dist_action_kl": dist_action_kl,
+                    "td_loss_est": float(td_loss / 2.0) if np.isfinite(td_loss) else float("nan"),
+                }
 
             log_row = {
                 "iter": n,
@@ -476,6 +562,8 @@ def train_unfixed_ac(config: Dict[str, Any]) -> Dict[str, Any]:
                 "delta_w_norm": delta_w_norm,
                 **probe_defaults,
             }
+            if extra_metrics:
+                log_row.update(extra_metrics)
             probe_updates = probe_manager.maybe_run(
                 iteration=n,
                 td_loss=td_loss,
diff --git a/tdrl_unfixed_ac/algos/unfixed_ac.py b/tdrl_unfixed_ac/algos/unfixed_ac.py
index ca85193..79fc72e 100644
--- a/tdrl_unfixed_ac/algos/unfixed_ac.py
+++ b/tdrl_unfixed_ac/algos/unfixed_ac.py
@@ -44,6 +44,7 @@ class LinearGaussianPolicy:
     theta: np.ndarray
     sigma: float
     v_max: float
+    squash_action: bool = False
 
     def mean(self, psi: np.ndarray) -> np.ndarray:
         return policy_mean(self.theta, psi)
@@ -60,10 +61,14 @@ class LinearGaussianPolicy:
         rng = rng if rng is not None else np.random.default_rng()
         mean = self.mean(psi)
         u = rng.normal(loc=mean, scale=self.sigma, size=self.action_dim)
-        return _squash_action(u, self.v_max)
+        if self.squash_action:
+            return _squash_action(u, self.v_max)
+        return u
 
     def pre_squash(self, action: np.ndarray) -> np.ndarray:
         action = np.asarray(action, dtype=float).reshape(self.action_dim)
+        if not self.squash_action:
+            return action
         return _unsquash_action(action, self.v_max)
 
     def log_prob_pre_squash(self, u: np.ndarray, psi: np.ndarray) -> float:
@@ -74,6 +79,8 @@ class LinearGaussianPolicy:
     def log_prob(self, action: np.ndarray, psi: np.ndarray) -> float:
         action = np.asarray(action, dtype=float).reshape(self.action_dim)
         mean = self.mean(psi)
+        if not self.squash_action:
+            return _gaussian_log_prob(action, mean, self.sigma)
         u = _unsquash_action(action, self.v_max)
         log_base = _gaussian_log_prob(u, mean, self.sigma)
         log_det = float(np.sum(np.log(self.v_max) + _log1m_tanh2(u)))
@@ -84,7 +91,7 @@ class LinearGaussianPolicy:
         action = np.asarray(action, dtype=float).reshape(self.action_dim)
         psi = np.asarray(psi, dtype=float).reshape(self.actor_dim)
         mean = self.mean(psi)
-        u = _unsquash_action(action, self.v_max)
+        u = self.pre_squash(action)
         diff = u - mean
         scale = 1.0 / (self.sigma * self.sigma * np.sqrt(self.actor_dim))
         return np.outer(psi, diff) * scale
@@ -117,6 +124,13 @@ def critic_value(w: np.ndarray, phi: np.ndarray) -> float:
     return float(np.dot(w, phi) / np.sqrt(w.shape[0]))
 
 
+def batch_step_scale(trajectories: int, horizon: int) -> float:
+    """Compute 1 / (sqrt(B) * T) scaling for batch updates."""
+    b_val = max(int(trajectories), 1)
+    t_val = max(int(horizon), 1)
+    return 1.0 / (np.sqrt(b_val) * t_val)
+
+
 def project_to_ball(theta: np.ndarray, radius: float) -> np.ndarray:
     """Project theta onto L2 ball of given radius."""
     if radius <= 0.0:
diff --git a/tdrl_unfixed_ac/configs/default.yaml b/tdrl_unfixed_ac/configs/default.yaml
index f0c45e9..be88823 100644
--- a/tdrl_unfixed_ac/configs/default.yaml
+++ b/tdrl_unfixed_ac/configs/default.yaml
@@ -2,6 +2,7 @@
   "torus_size": 1.0,
   "dt": 0.05,
   "v_max": 0.4,
+  "clip_action": false,
   "v_ghost": 0.3,
   "sigma_env": 0.01,
   "sigma_ghost": 0.01,
diff --git a/tdrl_unfixed_ac/envs/torus_gg.py b/tdrl_unfixed_ac/envs/torus_gg.py
index be0c04a..3bfd3f5 100644
--- a/tdrl_unfixed_ac/envs/torus_gg.py
+++ b/tdrl_unfixed_ac/envs/torus_gg.py
@@ -69,6 +69,7 @@ class TorusGobletGhostEnv:
         self.p_type_positive = float(self.cfg["p_type_positive"])
         self.type_resample_p = float(self.cfg["type_resample_p"])
         self.use_teacher_reward = bool(self.cfg.get("use_teacher_reward", True))
+        self.clip_action = bool(self.cfg.get("clip_action", False))
         self.feature_dim = int(self.cfg.get("feature_dim", 128))
         self.actor_feature_dim = int(self.cfg.get("actor_feature_dim", 32))
         self.c_psi = float(self.cfg.get("c_psi", 1.0))
@@ -237,7 +238,7 @@ class TorusGobletGhostEnv:
         return np.where(mask, 1.0, -1.0)
 
     def _clip_action(self, action: np.ndarray) -> np.ndarray:
-        if self.v_max <= 0.0:
+        if not self.clip_action or self.v_max <= 0.0:
             return action
         return np.clip(action, -self.v_max, self.v_max)
 
diff --git a/tdrl_unfixed_ac/probes/common.py b/tdrl_unfixed_ac/probes/common.py
index ed3c6b5..0710de0 100644
--- a/tdrl_unfixed_ac/probes/common.py
+++ b/tdrl_unfixed_ac/probes/common.py
@@ -10,9 +10,9 @@ from tdrl_unfixed_ac.algos.unfixed_ac import LinearGaussianPolicy, apply_rho_cli
 from tdrl_unfixed_ac.envs.torus_gg import TorusGobletGhostEnv
 
 
-def clip_action(action: np.ndarray, v_max: float) -> np.ndarray:
-    """Clip action per component to [-v_max, v_max]."""
-    if v_max <= 0.0:
+def clip_action(action: np.ndarray, v_max: float, clip_action: bool) -> np.ndarray:
+    """Clip action per component to [-v_max, v_max] if enabled."""
+    if not clip_action or v_max <= 0.0:
         return action
     return np.clip(action, -v_max, v_max)
 
@@ -30,7 +30,7 @@ def mc_bar_phi(
     phis = []
     for _ in range(k_mc):
         action = policy.sample_action(psi, rng)
-        action = clip_action(action, env.v_max)
+        action = clip_action(action, env.v_max, env.clip_action)
         phis.append(env.compute_features(action)["phi"])
     return np.mean(np.stack(phis, axis=0), axis=0)
 
@@ -61,7 +61,7 @@ def collect_critic_batch(
 
     for _ in range(batch_size):
         a = mu_policy.sample_action(psi, rng)
-        a = clip_action(a, env.v_max)
+        a = clip_action(a, env.v_max, env.clip_action)
 
         logp_pi = pi_policy.log_prob(a, psi)
         logp_mu = mu_policy.log_prob(a, psi)
diff --git a/tdrl_unfixed_ac/probes/distribution_probe.py b/tdrl_unfixed_ac/probes/distribution_probe.py
index 52eb3b0..cb77294 100644
--- a/tdrl_unfixed_ac/probes/distribution_probe.py
+++ b/tdrl_unfixed_ac/probes/distribution_probe.py
@@ -31,7 +31,7 @@ def _collect_state_features(
         psi = features["psi"]
         psi_vecs.append(psi)
         action = policy.sample_action(psi, rng)
-        action = clip_action(action, env.v_max)
+        action = clip_action(action, env.v_max, env.clip_action)
         _, _, terminated, truncated, _ = env.step(action)
         if terminated or truncated:
             raise RuntimeError("Environment should be continuing but returned a terminal flag.")
@@ -60,7 +60,7 @@ def _collect_rho_samples(
     rhos = []
     for _ in range(num_samples):
         action = mu_policy.sample_action(psi, rng)
-        action = clip_action(action, env.v_max)
+        action = clip_action(action, env.v_max, env.clip_action)
         logp_pi = pi_policy.log_prob(action, psi)
         logp_mu = mu_policy.log_prob(action, psi)
         rho_raw = float(np.exp(logp_pi - logp_mu))
@@ -173,6 +173,7 @@ def run_distribution_probe(
     theta_pi: np.ndarray,
     sigma_mu: float,
     sigma_pi: float,
+    squash_action: bool,
     num_samples: int,
     seed: Optional[int],
     action_samples: int = 64,
@@ -188,8 +189,18 @@ def run_distribution_probe(
     env_pi = TorusGobletGhostEnv(config=env_config, rng=np.random.default_rng(base_seed + 13))
     env_rho = TorusGobletGhostEnv(config=env_config, rng=np.random.default_rng(base_seed + 17))
 
-    mu_policy = LinearGaussianPolicy(theta=np.array(theta_mu, copy=True), sigma=float(sigma_mu), v_max=env_mu.v_max)
-    pi_policy = LinearGaussianPolicy(theta=np.array(theta_pi, copy=True), sigma=float(sigma_pi), v_max=env_mu.v_max)
+    mu_policy = LinearGaussianPolicy(
+        theta=np.array(theta_mu, copy=True),
+        sigma=float(sigma_mu),
+        v_max=env_mu.v_max,
+        squash_action=bool(squash_action),
+    )
+    pi_policy = LinearGaussianPolicy(
+        theta=np.array(theta_pi, copy=True),
+        sigma=float(sigma_pi),
+        v_max=env_mu.v_max,
+        squash_action=bool(squash_action),
+    )
 
     obs_mu, psi_mu = _collect_state_features(env_mu, mu_policy, rng_mu, num_samples)
     obs_pi, psi_pi = _collect_state_features(env_pi, pi_policy, rng_pi, num_samples)
diff --git a/tdrl_unfixed_ac/probes/fixed_point_probe.py b/tdrl_unfixed_ac/probes/fixed_point_probe.py
index 00782d3..5dacd1b 100644
--- a/tdrl_unfixed_ac/probes/fixed_point_probe.py
+++ b/tdrl_unfixed_ac/probes/fixed_point_probe.py
@@ -19,6 +19,7 @@ def run_fixed_point_probe(
     w_init: np.ndarray,
     sigma_mu: float,
     sigma_pi: float,
+    squash_action: bool,
     alpha_w: float,
     gamma: float,
     k_mc: int,
@@ -33,8 +34,18 @@ def run_fixed_point_probe(
     rng = np.random.default_rng(seed)
     env = TorusGobletGhostEnv(config=env_config, rng=rng)
 
-    mu_policy = LinearGaussianPolicy(theta=np.array(theta_mu, copy=True), sigma=float(sigma_mu), v_max=env.v_max)
-    pi_policy = LinearGaussianPolicy(theta=np.array(theta_pi, copy=True), sigma=float(sigma_pi), v_max=env.v_max)
+    mu_policy = LinearGaussianPolicy(
+        theta=np.array(theta_mu, copy=True),
+        sigma=float(sigma_mu),
+        v_max=env.v_max,
+        squash_action=bool(squash_action),
+    )
+    pi_policy = LinearGaussianPolicy(
+        theta=np.array(theta_pi, copy=True),
+        sigma=float(sigma_pi),
+        v_max=env.v_max,
+        squash_action=bool(squash_action),
+    )
 
     batch = collect_critic_batch(
         env,
diff --git a/tdrl_unfixed_ac/probes/manager.py b/tdrl_unfixed_ac/probes/manager.py
index 3bc5692..550b0e3 100644
--- a/tdrl_unfixed_ac/probes/manager.py
+++ b/tdrl_unfixed_ac/probes/manager.py
@@ -33,6 +33,7 @@ class ProbeManager:
         k_mc: int,
         sigma_mu: float,
         sigma_pi: float,
+        squash_action: bool,
         rho_clip: Optional[float],
         disable_rho_clip: bool,
     ) -> None:
@@ -66,6 +67,7 @@ class ProbeManager:
         self.k_mc = int(k_mc)
         self.sigma_mu = float(sigma_mu)
         self.sigma_pi = float(sigma_pi)
+        self.squash_action = bool(squash_action)
         self.rho_clip = rho_clip
         self.disable_rho_clip = bool(disable_rho_clip)
 
@@ -143,6 +145,7 @@ class ProbeManager:
                     w_init=w,
                     sigma_mu=self.sigma_mu,
                     sigma_pi=self.sigma_pi,
+                    squash_action=self.squash_action,
                     alpha_w=float(fixed_cfg["alpha_w"]),
                     gamma=float(fixed_cfg["gamma"]),
                     k_mc=int(fixed_cfg["k_mc"]),
@@ -204,6 +207,7 @@ class ProbeManager:
                     theta_pi=theta_pi,
                     sigma_mu=self.sigma_mu,
                     sigma_pi=self.sigma_pi,
+                    squash_action=self.squash_action,
                     alpha_w=float(stability_cfg["alpha_w"]),
                     train_step_scale=self.train_step_scale,
                     gamma=float(stability_cfg["gamma"]),
@@ -244,6 +248,7 @@ class ProbeManager:
                     theta_pi=theta_pi,
                     sigma_mu=self.sigma_mu,
                     sigma_pi=self.sigma_pi,
+                    squash_action=self.squash_action,
                     num_samples=int(dist_cfg["num_samples"]),
                     action_samples=int(dist_cfg["action_samples"]),
                     rho_clip=self.rho_clip,
diff --git a/tdrl_unfixed_ac/probes/stability_probe.py b/tdrl_unfixed_ac/probes/stability_probe.py
index 58c53d9..438a53d 100644
--- a/tdrl_unfixed_ac/probes/stability_probe.py
+++ b/tdrl_unfixed_ac/probes/stability_probe.py
@@ -18,6 +18,7 @@ def run_stability_probe(
     theta_pi: np.ndarray,
     sigma_mu: float,
     sigma_pi: float,
+    squash_action: bool,
     alpha_w: float,
     train_step_scale: float,
     gamma: float,
@@ -31,8 +32,18 @@ def run_stability_probe(
     """Estimate local amplification (spectral radius proxy) for critic updates."""
     rng = np.random.default_rng(seed)
     v_max = float(env_config.get("v_max", 1.0))
-    mu_policy = LinearGaussianPolicy(theta=np.array(theta_mu, copy=True), sigma=float(sigma_mu), v_max=v_max)
-    pi_policy = LinearGaussianPolicy(theta=np.array(theta_pi, copy=True), sigma=float(sigma_pi), v_max=v_max)
+    mu_policy = LinearGaussianPolicy(
+        theta=np.array(theta_mu, copy=True),
+        sigma=float(sigma_mu),
+        v_max=v_max,
+        squash_action=bool(squash_action),
+    )
+    pi_policy = LinearGaussianPolicy(
+        theta=np.array(theta_pi, copy=True),
+        sigma=float(sigma_pi),
+        v_max=v_max,
+        squash_action=bool(squash_action),
+    )
 
     def _estimate_proxy(batch: Dict[str, np.ndarray], local_rng: np.random.Generator) -> float:
         phi = batch["phi"]
diff --git a/tdrl_unfixed_ac/reporting/run_report.py b/tdrl_unfixed_ac/reporting/run_report.py
index 446ce25..48f485a 100644
--- a/tdrl_unfixed_ac/reporting/run_report.py
+++ b/tdrl_unfixed_ac/reporting/run_report.py
@@ -317,10 +317,9 @@ def _train_step_scale(cfg: Dict[str, Any]) -> Optional[float]:
         horizon = int(cfg.get("horizon", 0) or 0)
     except (TypeError, ValueError):
         return None
-    total_steps = max(trajectories * horizon, 0)
-    if alpha_w <= 0.0 or total_steps <= 0:
+    if alpha_w <= 0.0 or trajectories <= 0 or horizon <= 0:
         return None
-    return alpha_w / total_steps
+    return alpha_w / (math.sqrt(trajectories) * horizon)
 
 
 def _get_git_commit(run_dir: Path) -> Optional[str]:
@@ -559,6 +558,39 @@ def _health_checks(
             "applicable": True,
         }
 
+    # sigma_condition
+    sigma_mu = cfg.get("sigma_mu")
+    sigma_pi = cfg.get("sigma_pi")
+    try:
+        sigma_mu_val = float(sigma_mu) if sigma_mu is not None else None
+        sigma_pi_val = float(sigma_pi) if sigma_pi is not None else None
+    except (TypeError, ValueError):
+        sigma_mu_val = None
+        sigma_pi_val = None
+    if sigma_mu_val is None or sigma_pi_val is None or sigma_mu_val <= 0.0 or sigma_pi_val <= 0.0:
+        checks["sigma_condition"] = {
+            "pass": True,
+            "reason": "sigma parameters unavailable",
+            "observed": {"sigma_mu": sigma_mu, "sigma_pi": sigma_pi},
+            "applicable": False,
+        }
+    else:
+        lhs = sigma_pi_val * sigma_pi_val
+        rhs = 2.0 * sigma_mu_val * sigma_mu_val
+        passed = lhs < rhs
+        reason = "sigma_pi^2 < 2 sigma_mu^2" if passed else "sigma_pi^2 >= 2 sigma_mu^2"
+        checks["sigma_condition"] = {
+            "pass": passed,
+            "reason": reason,
+            "observed": {
+                "sigma_mu": sigma_mu_val,
+                "sigma_pi": sigma_pi_val,
+                "sigma_pi2": lhs,
+                "two_sigma_mu2": rhs,
+            },
+            "applicable": True,
+        }
+
     # on_policy_expected
     check_name = cfg.get("check_name")
     applicable = _is_on_policy_run(cfg)
