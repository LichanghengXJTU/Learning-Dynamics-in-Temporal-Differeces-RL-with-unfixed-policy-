{"run_id": "b0p005_aw0p02_tmos0_s0_b9094ba9", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0_s0_b9094ba9", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.0", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2421.082752943039, "timestamp": "2025-12-30T01:26:26"}
{"run_id": "b0p005_aw0p02_tmos0p05556_s0_eb49491a", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p05556_s0_eb49491a", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.055555555556", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2418.872637987137, "timestamp": "2025-12-30T02:06:45"}
{"run_id": "b0p005_aw0p02_tmos0p1111_s0_2dd01e8d", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p1111_s0_2dd01e8d", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.111111111111", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2420.68270277977, "timestamp": "2025-12-30T02:47:06"}
{"run_id": "b0p005_aw0p02_tmos0p1667_s0_f2503c1d", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p1667_s0_f2503c1d", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.166666666667", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2417.985430955887, "timestamp": "2025-12-30T03:27:24"}
{"run_id": "b0p005_aw0p02_tmos0p2222_s0_d86bfa49", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p2222_s0_d86bfa49", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.222222222222", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2424.0078608989716, "timestamp": "2025-12-30T04:07:48"}
{"run_id": "b0p005_aw0p02_tmos0p2778_s0_4bd77e92", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p2778_s0_4bd77e92", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.277777777778", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2416.9534199237823, "timestamp": "2025-12-30T04:48:05"}
{"run_id": "b0p005_aw0p02_tmos0p3333_s0_fcb3c09a", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p3333_s0_fcb3c09a", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.333333333333", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2418.2994899749756, "timestamp": "2025-12-30T05:28:23"}
{"run_id": "b0p005_aw0p02_tmos0p3889_s0_b8c731fc", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p3889_s0_b8c731fc", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.388888888889", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2417.7454509735107, "timestamp": "2025-12-30T06:08:41"}
{"run_id": "b0p005_aw0p02_tmos0p4444_s0_c866ce21", "command": ["/Users/enhuili/Desktop/Learning Dynamics in Temporal Differences Reinforcement Learning with Unfixed Policy/Learning-Dynamics-in-Temporal-Differeces-RL-with-unfixed-policy-/.venv/bin/python", "scripts/run_train.py", "--config", "configs/train_plateau.yaml", "--output-dir", "outputs/base_check/20251230_004605/sweep/runs/b0p005_aw0p02_tmos0p4444_s0_c866ce21", "--beta", "0.005", "--alpha-w", "0.02", "--theta-mu-offset-scale", "0.444444444444", "--outer-iters", "200", "--report-every", "50", "--seed", "0", "--resume"], "returncode": 0, "duration_sec": 2421.023673057556, "timestamp": "2025-12-30T06:49:02"}
